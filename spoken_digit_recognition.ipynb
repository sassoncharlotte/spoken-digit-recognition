{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spoken Digits Recognition\n",
    "\n",
    "What happens in your mobile phone when you talk to Siri or Alexa ?\n",
    "It is the question that interested us for this topic. \n",
    "\n",
    "We want you at the end of this notebook to have an idea of how something you use everyday (Speech Recognition) works.\n",
    "\n",
    "In this Jupyter we simplify the problem. We focus on detecting spoken digits from 2 to 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages required :\n",
    "- librosa\n",
    "- tensorflow\n",
    "- sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import librosa\n",
    "import numpy as np\n",
    "import librosa.display\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Flatten\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsize = 64 #batch size\n",
    "audio_features = 20\n",
    "utterance_length =25  # Modify to see what different results you can get (35 init)\n",
    "ndigits = 10 #number of digits\n",
    "n_mfcc=20\n",
    "train_path = 'data/recordings/train/' #path to the folder that contains the training data\n",
    "prediction_path = 'data/recordings/test/' #path to the folder that contains the data for prediction\n",
    "nb_epochs = 100 #the number of epochs when training the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions that extract the Mel Frequency Cepstrum Coefficients (MFCC) of the data\n",
    "\n",
    "MFCC are the audio features the most used in speech recognition because they try to integrate the way a human perceive sounds.\n",
    "\n",
    "Here is a definition of the MFCC given by Wikipedia :\n",
    "\n",
    "*In sound processing, the mel-frequency cepstrum (MFC) is a representation of the short-term power spectrum of a sound, based on a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency.*\n",
    "\n",
    "*Mel-frequency cepstral coefficients (MFCCs) are coefficients that collectively make up an MFC. They are derived from a type of cepstral representation of the audio clip (a nonlinear \"spectrum-of-a-spectrum\"). The difference between the cepstrum and the mel-frequency cepstrum is that in the MFC, the frequency bands are equally spaced on the mel scale, which approximates the human auditory system's response more closely than the linearly-spaced frequency bands used in the normal cepstrum. This frequency warping can allow for better representation of sound, for example, in audio compression.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(folder_path, utterance_length, n_mfcc):\n",
    "    # Get raw .wav data and sampling rate (from librosa's load function) : Load an audio file as a floating point time series\n",
    "    raw_w, sampling_rate = librosa.load(folder_path, mono=True)\n",
    "    \n",
    "    # Obtain MFCC Features from raw data : Mnp.ndarray [shape=(n_mfcc, t)]\n",
    "    mfcc_features = librosa.feature.mfcc(raw_w, sr = sampling_rate, n_fft = 2048, n_mfcc = n_mfcc)\n",
    "    \n",
    "    if mfcc_features.shape[1] > utterance_length:\n",
    "        #print(\"ok\")\n",
    "        mfcc_features = mfcc_features[:, 0:utterance_length] #we then stop at utterance_length for the columns of the array\n",
    "    else:\n",
    "        #pad the array\n",
    "        mfcc_features = np.pad(mfcc_features, ((0, 0), (0, utterance_length - mfcc_features.shape[1])),\n",
    "                               mode='constant', constant_values=0)\n",
    "    \n",
    "    return mfcc_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc_batch(folder_path, batch_size, utterance_length):\n",
    "    print(\"Loading the data...\\nIt will take less than a minute\")\n",
    "    files = os.listdir(folder_path) #a list containing the names of the entries in the directory given by path\n",
    "    X = [] #ensemble des mfcc\n",
    "    y = []\n",
    "    spoken_digits = []\n",
    "\n",
    "    while True:\n",
    "        # Shuffle Files\n",
    "        random.shuffle(files)\n",
    "        for fname in files:\n",
    "            spoken_digit = int(fname[0])\n",
    "            spoken_digits += [spoken_digit]\n",
    "            \n",
    "            # Make sure file is a .wav file\n",
    "            if not fname.endswith(\".wav\"):\n",
    "                continue\n",
    "            \n",
    "            # Get MFCC Features for the file\n",
    "            mfcc_features = extract_mfcc(folder_path + fname, utterance_length, n_mfcc)\n",
    "            \n",
    "            # One-hot encode label for 10 digits 0-9\n",
    "            label = np.eye(10)[spoken_digit]\n",
    "            \n",
    "            # Append to label batch\n",
    "            y.append(label)\n",
    "            \n",
    "            # Append mfcc features to ft_batch\n",
    "            X.append(mfcc_features)\n",
    "        \n",
    "        print(\"Data loaded\")\n",
    "        \n",
    "        return np.array(X), np.array(y), spoken_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of a Mel Frequency Cepstrum (MFC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the Mel Frequency Cepstrum of a random audio, taken in the database. Execute several times this cell to see different types of MFC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the audios in the database don't have the same length, we added a padding to put every MFC at the same shape. It is why, with a lot of audios, we have a zone with nothing relevant on the right of the cepstrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(train_path)\n",
    "random.shuffle(files)\n",
    "fname = files[0] #a random file in our database\n",
    "\n",
    "mfccs = extract_mfcc(train_path + fname, utterance_length, n_mfcc)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "img = librosa.display.specshow(mfccs, x_axis='time', ax=ax)\n",
    "fig.colorbar(img, ax=ax)\n",
    "ax.set(title='MFC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's build and train our neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a neural network with three hidden fully connected layers and another fully connected layer for the output. The first step is to flatten the data so that is can fit the first hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten())  #3D arrays => 1D\n",
    "\n",
    "#adding 3 hidden fully connected layers with ReLU activation fonction\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "#output layer\n",
    "model.add(Dense(ndigits, activation='softmax'))\n",
    "\n",
    "#compiling the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must load the data that we are going to use to train our neural network, using the name X for the lists of MFCCs and y for the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading X (the list of MFCCs) and y (the list of labels)\n",
    "(X, y, spoken_digits) = get_mfcc_batch(train_path, 256, utterance_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We separate the data into two parts : the training data and the testing data. Then, we train our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separation between training data (70%) and testing data (30%)\n",
    "X_tr,X_test,y_tr,y_test = train_test_split(X,y,test_size = 0.3)\n",
    "\n",
    "\n",
    "#training the model\n",
    "history = model.fit(X_tr, y_tr, epochs=nb_epochs, batch_size=bsize, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look of the summary of the network to see if it has been created the way we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a print of the summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vizualisation of the evolution of accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss']) \n",
    "plt.title('Model loss') \n",
    "plt.ylabel('Loss') \n",
    "plt.xlabel('Epoch') \n",
    "plt.legend(['Train', 'Test'], loc='upper left') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model/model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model saved on the files *model.h5* and *model.json*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('model/model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model/model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.summary() #visualisation of the architecture of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does our neural network will recognize audios that he has never \"heard\" before ? We are going to give him some new audios and see if he recognize the right digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the validation data\n",
    "X, y, spoken_digits = get_mfcc_batch(prediction_path, 256, utterance_length)\n",
    "\n",
    "#getting the prediction that our neural network gives us\n",
    "prediction_digits = loaded_model.predict_classes(X)\n",
    "\n",
    "accuracy = 0\n",
    "total = len(prediction_digits)\n",
    "\n",
    "for i in range (total):\n",
    "    prediction_digit = prediction_digits[i]\n",
    "    spoken_digit = spoken_digits[i]\n",
    "    if int(prediction_digit) == int(spoken_digit) :\n",
    "        accuracy += 100/30\n",
    "    print('Prediction : ' + str(prediction_digit) + ' |Â Right spoken digit : ' + str(spoken_digit) + '               => accuracy : ' + str(int(accuracy)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jason Brownlee, Your First Deep Learning Project in Python with Keras Step-By-Step, Deep Learning, 2019, available at https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/.\n",
    "\n",
    "Jason Brownlee, How to Save and Load Your Keras Deep Learning Model, Deep Learning, 2019, available at https://machinelearningmastery.com/save-load-keras-deep-learning-models/.\n",
    "\n",
    "Mohsin Baig, spoken-digit-recognition, available at https://github.com/moebg/spoken-digit-recognition/tree/master/data.\n",
    "\n",
    "Adhish Thite, Digit Recognition from Sound, available at https://adhishthite.github.io/sound-mnist/.\n",
    "\n",
    "Sanchit Tanwar, Building our first neural network in keras, 2019, available at https://towardsdatascience.com/building-our-first-neural-network-in-keras-bdc8abbc17f5."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
